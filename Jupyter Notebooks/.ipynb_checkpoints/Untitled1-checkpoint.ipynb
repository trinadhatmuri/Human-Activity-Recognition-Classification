{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ff4ed9c",
   "metadata": {},
   "source": [
    "<center><h1>Modelling and Evaluation</h1></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "be33883b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import RFE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e1d90b",
   "metadata": {},
   "source": [
    "<h2>Importing Datasets</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3853d855",
   "metadata": {},
   "source": [
    "<h4>Dataset with all features</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6eb4076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"../Processed Data/All Features/X_train.csv\")\n",
    "X_test = pd.read_csv(\"../Processed Data/All Features/X_test.csv\")\n",
    "Y_train = pd.read_csv(\"../Processed Data/All Features/Y_train.csv\")\n",
    "Y_test = pd.read_csv(\"../Processed Data/All Features/Y_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1544dbc0",
   "metadata": {},
   "source": [
    "We can only feed the activity codes to our model, so we single out the activity codes and use it as y_train and y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0745c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = Y_train[\"activity_code\"] - 1\n",
    "y_test = Y_test[\"activity_code\"] - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb792794",
   "metadata": {},
   "source": [
    "<h4>Dataset with features that are mean values</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a541eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mean = pd.read_csv(\"../Processed Data/Mean Features/feature_reduced_X_train.csv\")\n",
    "X_test_mean = pd.read_csv(\"../Processed Data/Mean Features/feature_reduced_X_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c7f32f",
   "metadata": {},
   "source": [
    "<h4>Dataset with PCA components that explain 95% variance</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66ac5a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = pd.read_csv(\"../Processed Data/PCA sets/X_train_pca_95.csv\")\n",
    "X_test_pca = pd.read_csv(\"../Processed Data/PCA sets/X_test_pca_95.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daeb975",
   "metadata": {},
   "source": [
    "<h4>Dataset based on top 50 percent of ANOVA F-test results, further filtered based on correlation</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56af9a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_anova = pd.read_csv(\"../Processed Data/ANOVA hybrid set/X_train_anova_filtered.csv\")\n",
    "X_test_anova = pd.read_csv(\"../Processed Data/ANOVA hybrid set/X_test_anova_filtered.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6701f8f",
   "metadata": {},
   "source": [
    "<h2>Training and Evaluation function</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63f101f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_test, y_test, model_name, feature_set_name):\n",
    "    \"\"\"\n",
    "    Trains, evaluates, and prints comprehensive metrics for a model.\n",
    "    Returns a dictionary with all results.\n",
    "    \"\"\"\n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    # Cross-validation for more robust estimate\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    cv_mean = cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    \n",
    "    # Create results dictionary\n",
    "    results = {\n",
    "        'model': model_name,\n",
    "        'feature_set': feature_set_name,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'cv_mean': cv_mean,\n",
    "        'cv_std': cv_std,\n",
    "        'num_features': X_train.shape[1]\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Results for {model_name} on {feature_set_name}:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")\n",
    "    print(f\"CV Accuracy: {cv_mean:.4f} (±{cv_std:.4f})\")\n",
    "    print(f\"Number of features: {X_train.shape[1]}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Initialize list to store all results\n",
    "all_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa956a6",
   "metadata": {},
   "source": [
    "The <b>evaluate_model</b> function is going to train and evaluate both Random Forest and XGBoost models on 4 different feature sets based on various evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e01d712",
   "metadata": {},
   "source": [
    "So now what we are going to do is feed the models we want to train and on the feature sets we want to train them to our evaluate_model function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33d0b7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiments...\n",
      "Total combinations: 2 models × 4 feature sets = 8 experiments\n",
      "\n",
      "==================================================\n",
      "Results for Random_Forest on A_All_Features:\n",
      "==================================================\n",
      "Accuracy:  0.9257\n",
      "Precision: 0.9270\n",
      "Recall:    0.9257\n",
      "F1-Score:  0.9255\n",
      "CV Accuracy: 0.9172 (±0.0164)\n",
      "Number of features: 561\n",
      "\n",
      "==================================================\n",
      "Results for Random_Forest on B_Mean_Features:\n",
      "==================================================\n",
      "Accuracy:  0.9270\n",
      "Precision: 0.9283\n",
      "Recall:    0.9270\n",
      "F1-Score:  0.9269\n",
      "CV Accuracy: 0.9227 (±0.0171)\n",
      "Number of features: 66\n",
      "\n",
      "==================================================\n",
      "Results for Random_Forest on C_PCA_Features:\n",
      "==================================================\n",
      "Accuracy:  0.8880\n",
      "Precision: 0.8925\n",
      "Recall:    0.8880\n",
      "F1-Score:  0.8868\n",
      "CV Accuracy: 0.8598 (±0.0171)\n",
      "Number of features: 102\n",
      "\n",
      "==================================================\n",
      "Results for Random_Forest on D_ANOVA_Filtered:\n",
      "==================================================\n",
      "Accuracy:  0.9046\n",
      "Precision: 0.9077\n",
      "Recall:    0.9046\n",
      "F1-Score:  0.9048\n",
      "CV Accuracy: 0.9094 (±0.0211)\n",
      "Number of features: 86\n",
      "\n",
      "==================================================\n",
      "Results for XGBoost on A_All_Features:\n",
      "==================================================\n",
      "Accuracy:  0.9393\n",
      "Precision: 0.9402\n",
      "Recall:    0.9393\n",
      "F1-Score:  0.9391\n",
      "CV Accuracy: 0.9214 (±0.0205)\n",
      "Number of features: 561\n",
      "\n",
      "==================================================\n",
      "Results for XGBoost on B_Mean_Features:\n",
      "==================================================\n",
      "Accuracy:  0.9074\n",
      "Precision: 0.9090\n",
      "Recall:    0.9074\n",
      "F1-Score:  0.9075\n",
      "CV Accuracy: 0.9055 (±0.0183)\n",
      "Number of features: 66\n",
      "\n",
      "==================================================\n",
      "Results for XGBoost on C_PCA_Features:\n",
      "==================================================\n",
      "Accuracy:  0.9114\n",
      "Precision: 0.9144\n",
      "Recall:    0.9114\n",
      "F1-Score:  0.9105\n",
      "CV Accuracy: 0.8757 (±0.0162)\n",
      "Number of features: 102\n",
      "\n",
      "==================================================\n",
      "Results for XGBoost on D_ANOVA_Filtered:\n",
      "==================================================\n",
      "Accuracy:  0.9063\n",
      "Precision: 0.9080\n",
      "Recall:    0.9063\n",
      "F1-Score:  0.9063\n",
      "CV Accuracy: 0.9127 (±0.0189)\n",
      "Number of features: 86\n",
      "\n",
      "============================================================\n",
      "ALL EXPERIMENTS COMPLETED!\n",
      "============================================================\n",
      "\n",
      "Summary of all results:\n",
      "        model      feature_set  accuracy  f1_score  num_features\n",
      "Random_Forest   A_All_Features  0.925687  0.925544           561\n",
      "Random_Forest  B_Mean_Features  0.927044  0.926894            66\n",
      "Random_Forest   C_PCA_Features  0.888022  0.886814           102\n",
      "Random_Forest D_ANOVA_Filtered  0.904649  0.904786            86\n",
      "      XGBoost   A_All_Features  0.939260  0.939068           561\n",
      "      XGBoost  B_Mean_Features  0.907363  0.907517            66\n",
      "      XGBoost   C_PCA_Features  0.911435  0.910531           102\n",
      "      XGBoost D_ANOVA_Filtered  0.906345  0.906322            86\n"
     ]
    }
   ],
   "source": [
    "feature_sets = {\n",
    "    'A_All_Features': (X_train, X_test),\n",
    "    'B_Mean_Features': (X_train_mean, X_test_mean),\n",
    "    'C_PCA_Features': (X_train_pca, X_test_pca),\n",
    "    'D_ANOVA_Filtered': (X_train_anova, X_test_anova)\n",
    "}\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "xgb_model = XGBClassifier(n_estimators=100, random_state=42, n_jobs=-1, eval_metric='mlogloss')\n",
    "\n",
    "models = {\n",
    "    'Random_Forest': rf_model,\n",
    "    'XGBoost': xgb_model\n",
    "}\n",
    "\n",
    "# Initialize list to store all results\n",
    "all_results = []\n",
    "\n",
    "print(\"Starting experiments...\")\n",
    "print(f\"Total combinations: {len(models)} models × {len(feature_sets)} feature sets = {len(models) * len(feature_sets)} experiments\")\n",
    "\n",
    "# Run all combinations\n",
    "for model_name, model in models.items():\n",
    "    for feature_name, (X_train_set, X_test_set) in feature_sets.items():\n",
    "        # Use the evaluate_model function we already created\n",
    "        results = evaluate_model(\n",
    "            model=model, \n",
    "            X_train=X_train_set, \n",
    "            y_train=y_train,  # Should be the same for all\n",
    "            X_test=X_test_set, \n",
    "            y_test=y_test,    # Should be the same for all\n",
    "            model_name=model_name, \n",
    "            feature_set_name=feature_name\n",
    "        )\n",
    "        all_results.append(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL EXPERIMENTS COMPLETED!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Convert results to DataFrame for easy analysis\n",
    "results_df = pd.DataFrame(all_results)\n",
    "print(\"\\nSummary of all results:\")\n",
    "print(results_df[['model', 'feature_set', 'accuracy', 'f1_score', 'num_features']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdd051e",
   "metadata": {},
   "source": [
    "<h3>Observations</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ccdfaf",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Since our original dataset is not skewed for any single activity, accuracy is a good metric to evaluate our models performance on</li>\n",
    "    <li>The feature set with all the 561 features trained with the XGBoost model is the best model we have and can say that it is for Random Forest model too. This models trained with this feature set are going to be our <b>Baseline models</b> for comparision with other feature sets.</li>\n",
    "    <li>The best performing model with diminished feature set (just 11 percent of features from the original dataset) is going to be the feature set with only mean features, which looks like it performed almost on par with the baseline model for Random Forest while it performed very well with XGBoost too.</li>\n",
    "    <li>The PCA components also performed very well with accuracy of almost 89 percent and 91 percent for Random Forest and XGBoost models respectively</li>\n",
    "    <li>The feature set we got based on ANOVA F-test score and further filtering based on correlation perform very well with just over 90 percent accuracy for both our models</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefe2e04",
   "metadata": {},
   "source": [
    "<h2>Recursive Feature Elimination</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d730373",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcfe6de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcedf90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de44e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "518df5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Evaluating RFECV-selected features...\n",
      "\n",
      "==================================================\n",
      "Results for XGBoost on E_RFE_Reported:\n",
      "==================================================\n",
      "Accuracy:  0.9063\n",
      "Precision: 0.9080\n",
      "Recall:    0.9063\n",
      "F1-Score:  0.9063\n",
      "CV Accuracy: 0.9127 (±0.0189)\n",
      "Number of features: 86\n",
      "\n",
      "2. Evaluating truly optimal features...\n",
      "\n",
      "==================================================\n",
      "Results for XGBoost on E_RFE_Optimal_True:\n",
      "==================================================\n",
      "Accuracy:  0.8996\n",
      "Precision: 0.9008\n",
      "Recall:    0.8996\n",
      "F1-Score:  0.8994\n",
      "CV Accuracy: 0.9071 (±0.0227)\n",
      "Number of features: 27\n",
      "\n",
      "============================================================\n",
      "COMPARISON RESULTS\n",
      "============================================================\n",
      "RFECV Reported: 86 features, Accuracy: 0.9063\n",
      "True Optimal: 27 features, Accuracy: 0.8996\n",
      "Feature reduction: 86 → 27 features (68.6% reduction)\n",
      "\n",
      "Final summary of all results (without duplication):\n",
      "        model        feature_set  accuracy  f1_score  num_features\n",
      "Random_Forest     A_All_Features  0.925687  0.925544           561\n",
      "Random_Forest    B_Mean_Features  0.927044  0.926894            66\n",
      "Random_Forest     C_PCA_Features  0.888022  0.886814           102\n",
      "Random_Forest   D_ANOVA_Filtered  0.904649  0.904786            86\n",
      "      XGBoost     A_All_Features  0.939260  0.939068           561\n",
      "      XGBoost    B_Mean_Features  0.907363  0.907517            66\n",
      "      XGBoost     C_PCA_Features  0.911435  0.910531           102\n",
      "      XGBoost   D_ANOVA_Filtered  0.906345  0.906322            86\n",
      "      XGBoost     E_RFE_Reported  0.906345  0.906322            86\n",
      "      XGBoost E_RFE_Optimal_True  0.899559  0.899450            27\n"
     ]
    }
   ],
   "source": [
    "xgb_model = XGBClassifier(n_estimators=100, random_state=42, eval_metric='mlogloss')\n",
    "\n",
    "# Create and fit RFECV\n",
    "min_features_to_select = 20\n",
    "cv = StratifiedKFold(5)\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=xgb_model,\n",
    "    step=10,\n",
    "    cv=cv,\n",
    "    scoring='accuracy',\n",
    "    min_features_to_select=min_features_to_select,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Fitting RFECV (this may take a while...)\")\n",
    "rfecv.fit(X_train_anova, y_train)\n",
    "print(\"RFECV fitting completed!\")\n",
    "\n",
    "# Plot the RFECV results\n",
    "plt.figure(figsize=(10, 6))\n",
    "x_values = range(min_features_to_select, len(rfecv.cv_results_['mean_test_score']) + min_features_to_select)\n",
    "plt.plot(x_values, rfecv.cv_results_['mean_test_score'])\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (accuracy)\")\n",
    "plt.title(\"RFECV Performance vs Number of Features\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Find the true optimal number of features from CV scores\n",
    "cv_scores = rfecv.cv_results_['mean_test_score']\n",
    "optimal_num_features = np.argmax(cv_scores) + min_features_to_select\n",
    "max_cv_score = np.max(cv_scores)\n",
    "\n",
    "print(f\"RFECV reported optimal features: {rfecv.n_features_}\")\n",
    "print(f\"True optimal number of features from CV: {optimal_num_features}\")\n",
    "print(f\"Maximum CV accuracy: {max_cv_score:.4f}\")\n",
    "\n",
    "# Create both feature sets for comparison\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CREATING AND EVALUATING BOTH FEATURE SETS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Set 1: RFECV-reported features (86 features)\n",
    "X_train_reported = X_train_anova.iloc[:, rfecv.support_]\n",
    "X_test_reported = X_test_anova.iloc[:, rfecv.support_]\n",
    "\n",
    "# Set 2: True optimal features (27 features)\n",
    "optimal_rfe = RFE(estimator=xgb_model, n_features_to_select=optimal_num_features)\n",
    "optimal_rfe.fit(X_train_anova, y_train)\n",
    "X_train_optimal = X_train_anova.iloc[:, optimal_rfe.support_]\n",
    "X_test_optimal = X_test_anova.iloc[:, optimal_rfe.support_]\n",
    "\n",
    "true_optimal_features = X_train_anova.columns[optimal_rfe.support_].tolist()\n",
    "print(f\"True optimal features ({len(true_optimal_features)}):\")\n",
    "for i, feat in enumerate(true_optimal_features[:15]):\n",
    "    print(f\"  {i+1}. {feat}\")\n",
    "if len(true_optimal_features) > 15:\n",
    "    print(f\"  ... and {len(true_optimal_features) - 15} more\")\n",
    "\n",
    "# Evaluate both feature sets\n",
    "new_results = []\n",
    "\n",
    "# 1. Evaluate RFECV-reported features\n",
    "print(\"\\n1. Evaluating RFECV-reported features...\")\n",
    "reported_results = evaluate_model(\n",
    "    model=XGBClassifier(n_estimators=100, random_state=42, eval_metric='mlogloss'),\n",
    "    X_train=X_train_reported,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test_reported,\n",
    "    y_test=y_test,\n",
    "    model_name='XGBoost',\n",
    "    feature_set_name='E_RFE_Reported'\n",
    ")\n",
    "new_results.append(reported_results)\n",
    "\n",
    "# 2. Evaluate truly optimal features\n",
    "print(\"\\n2. Evaluating truly optimal features...\")\n",
    "optimal_results = evaluate_model(\n",
    "    model=XGBClassifier(n_estimators=100, random_state=42, eval_metric='mlogloss'),\n",
    "    X_train=X_train_optimal,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test_optimal,\n",
    "    y_test=y_test,\n",
    "    model_name='XGBoost',\n",
    "    feature_set_name='E_RFE_Optimal_True'\n",
    ")\n",
    "new_results.append(optimal_results)\n",
    "\n",
    "# Compare results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"RFECV Reported: {rfecv.n_features_} features, Accuracy: {reported_results['accuracy']:.4f}\")\n",
    "print(f\"True Optimal: {optimal_num_features} features, Accuracy: {optimal_results['accuracy']:.4f}\")\n",
    "\n",
    "# Add to main results and display final summary\n",
    "all_results.extend(new_results)\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "print(\"\\nFinal summary of all results:\")\n",
    "final_summary = results_df[['model_name', 'feature_set_name', 'accuracy', 'f1_score', 'num_features']].drop_duplicates()\n",
    "print(final_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3990f9b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
